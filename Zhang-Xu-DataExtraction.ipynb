{"cells":[{"cell_type":"markdown","source":["## MIE1512\nDownload the raw event data of January 2017 and transform it to desired csv file.\n======"],"metadata":{}},{"cell_type":"markdown","source":["The datasets I need is the number of events a user performed in a given period. However, it can not be directly accessed by the Github API. The only source that I found to be appropriate for my research is the Github Archive, which records every single event happened since 2/12/2011. Thus, I chose to download all the events happend in January of 2017 and try to transfer them into the format of number of events users performed in a csv file.\n\nBecase the data on githubArchive are stored every hour, I need to download all the data in January, which are about 720 json files with total size about 80GB. What I need from the raw data are the numbers of events that a user has done on github in January. To achieve this, I merged the json files day by day and then extract the userID, event type for each record from each merged json file. Then I grouped by the user and count the number of each event type, used the pivot method to obtain the number of event a user performed for that day. Finally I union 31 days' records together and write them into a csv file.\n\nThe final dataframe Jan_df is downloaded, divided into 2 parts and uploaded to Github as the resource of the main notebook."],"metadata":{}},{"cell_type":"markdown","source":["Below are the functions used for the download, decompressing and merging these json files together."],"metadata":{}},{"cell_type":"code","source":["def add_zero(s):\n\n    if len(s) < 2:\n\n        return \"0\" + s\n\n    else:\n\n        return s"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import urllib\ndef downloadNDays(y, m, d, duration):\n    for i in range(duration):\n        Y= add_zero(str(y))\n        M= add_zero(str(m))\n        D= add_zero(str(d))\n        for i in range(24):\n            urllib.urlretrieve((\"http://data.githubarchive.org/%s-%s-%s-%s.json.gz\" %(Y, M, D, i)), (\"/tmp/%s-%s-%s-%s.json.gz\" %(Y, M, D, i)))\n        d= d+1\n## define the download functions to get data from github archive"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import dbutils\ndef deleteNDays(y, m, d, duration):\n    for i in range(duration):\n        Y= add_zero(str(y))\n        M= add_zero(str(m))\n        D= add_zero(str(d))\n        for i in range(24):\n            dbutils.fs.rm((\"/tmp/%s-%s-%s-%s.json.gz\" %(Y, M, D, i)))\n        dbutils.fs.rm((\"/tmp/%s-%s-%s-0.json\" %(Y, M, D)))\n        d= d+1"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["PATH_PREFIX= \"/tmp/\"\nEXT_JG=\".json.gz\"\nEXT_JSON= \".json\"\n\ndef get_file_name_json_gz(para_list):\n\n    return get_file_name(para_list) + EXT_JG\n\ndef get_file_name_json(para_list):\n\n    return get_file_name(para_list) + EXT_JSON\n\ndef get_file_name(para_list):\n\n    list_ = list(map(str,para_list))\n\n    list_[1] = add_zero(list_[1])    \n\n    list_[2] = add_zero(list_[2])\n\n    return '-'.join(list_)\n    \n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import gzip\ndef merge_one_day(cy, cm, cd):\n\n    tmp = []\n\n    for hh in range(24):\n\n        with gzip.open(PATH_PREFIX + get_file_name_json_gz([cy, cm, cd, hh]), 'r') as f_in:\n\n            tmp += f_in\n\n\n    with open(PATH_PREFIX + get_file_name_json([cy, cm, cd, 0]), 'wb') as f_out:\n\n       for line in tmp:\n\n           f_out.write(line)\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["get_one_day is used to get the dataframe of number of each user's event for one day"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions\nfrom pyspark.sql.functions import count\nfrom pyspark.sql.types import *\nPATH= 'file:/tmp/'\ndef get_one_day(cy, cm, cd):\n    inputPath = PATH + get_file_name_json([cy, cm, cd, 0])\n    print(inputPath)\n    InputDF = sqlContext.read.json(inputPath)\n\n    githubInput = InputDF.withColumn(\"actorID\", InputDF.actor.id)\n    githubInput = githubInput.withColumn(\"actorName\", InputDF.actor.login)\n    githubInput = githubInput.withColumn(\"repoId\", InputDF.repo.id)\n    githubInput = githubInput.withColumn(\"repoName\", InputDF.repo.Name)\n\n    githubInputDF= githubInput.select(\"id\",\"type\",\"actorID\",\"actorName\",\"repoID\",\"repoName\",\"created_at\")\n    \n    githubGroupDF= githubInputDF.groupBy(['actorID','actorName','type']).agg(count(\"id\").alias(\"_count\"))\n\n    typeList= githubGroupDF.select('type').distinct().rdd.map(lambda r: r[0]).collect() ## without actorID\n\n    typeList2= [str(x) for x in typeList]\n    type_list= []\n    type_list.extend(['actorID'])\n    type_list.extend(typeList2)\n\n    githubGroupedData= githubGroupDF.groupBy(['actorID','actorName']).pivot('type', typeList ).sum(\"_count\")\n\n    githubRdd= githubGroupedData.select(type_list).filter(\"PushEvent<100\").na.fill(0).rdd\n    \n    return githubRdd, type_list"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["get_n_days merge the 31 days' dataframe together."],"metadata":{}},{"cell_type":"code","source":["def get_n_days(cy, cm, cd, n):\n    nDays, type_list= get_one_day(cy, cm, cd)\n    for dd in range(cd+1,cd+n):\n        oneDayRdd, one_day_type= get_one_day(cy, cm, dd)\n        nDays = nDays.union(oneDayRdd)\n        type_list= type_list + list(set(one_day_type) - set(type_list))\n        print(\"get\"+ str(dd)+ \"day\")\n    return nDays, type_list"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Below is the main function and write all the data on a csv file."],"metadata":{}},{"cell_type":"code","source":["def mainDataCollection(cy, cm, sd, ed):\n    downloadNDays(cy, cm, sd, ed-sd+1)\n    print(\"Download Complete\")\n    for dd in range(sd, ed+1):\n        merge_one_day(cy,cm,dd)\n        print(\"merge %s\" %dd)\n    print(\"Merge Complete\")\n    ## decompress the data and put these json files into one json\n    nDaysRdd, type_list= get_n_days(cy, cm, sd, ed-sd+1)\n    nDaysRdd.toDF(type_list).write.csv((\"dbfs:/FileStore/tables/MIE1512/%s-%s/%s-%s-Start_%s-End_%s.csv\" %(cy, cm, cy, cm, sd, ed)), header= True)\n    print(\"csv writing complete\")\n    deleteNDays(cy, cm, sd, ed-sd+1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["mainDataCollection(2017,1,1,31)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Above codes cancatenate the data of each day together, but there are users that have records in many days of January.\n\nThus, I then group the users and sum the number of events for each user.\n\nAfter the group and sum operation, each row now represents a user and carries information about number of different events the user has done in January."],"metadata":{}},{"cell_type":"code","source":["JanDF = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('dbfs:/FileStore/tables/MIE1512/2017-1/2017-1-Start_1-End_31.csv')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["type_list= JanDF.schema.names[1:]\ntype_list1= JanDF.schema.names\nexprs = {x: \"sum\" for x in type_list}\ndf= JanDF.groupBy(\"actorID\").agg(exprs)\ndf.show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def get_within(column):\n    return column[column.find(\"(\")+1:column.find(\")\")]\n## return the column name within sum()\noldColumns= df.schema.names[1:]\nnewColumns= JanDF.schema.names[1:]\nJan_df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], get_within(oldColumns[idx])), xrange(len(oldColumns)), df)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["Jan_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(Jan_df)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"MIE 1512 version 3","notebookId":1857803565880758},"nbformat":4,"nbformat_minor":0}
